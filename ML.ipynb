{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "베스트 하이퍼 파라미터: {'learning_rate': 0.30000000000000004, 'max_depth': 3, 'max_features': 1, 'n_estimators': 5}\n",
      "베스트 하이퍼 파라미터 일 때 정확도: 0.96\n",
      "테스트세트에서의 정확도: 0.98\n",
      "실제 값: virginica, 예측 값: virginica\n",
      "실제 값: setosa, 예측 값: setosa\n",
      "실제 값: setosa, 예측 값: setosa\n",
      "실제 값: virginica, 예측 값: versicolor\n",
      "실제 값: versicolor, 예측 값: versicolor\n",
      "실제 값: versicolor, 예측 값: versicolor\n",
      "실제 값: virginica, 예측 값: virginica\n",
      "실제 값: versicolor, 예측 값: versicolor\n",
      "실제 값: virginica, 예측 값: virginica\n",
      "실제 값: setosa, 예측 값: setosa\n"
     ]
    }
   ],
   "source": [
    "#Gradient Boost algorithms\n",
    "#Stump나 tree가 아닌 하나의 leaf부터 시작\n",
    "# -*- coding:utf-8 -*-\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# 분류용 샘플 데이터 불러오기\n",
    "iris = load_iris()\n",
    "X, y, labels = iris.data, iris.target, iris.target_names\n",
    "\n",
    "# 학습/테스트 데이터셋 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1, stratify=y)\n",
    "\n",
    "# 데이터 전처리(표준화, Standardization)\n",
    "std = StandardScaler()\n",
    "X_train_std = std.fit_transform(X_train)\n",
    "X_test_std = std.transform(X_test)\n",
    "\n",
    "# 그래디언트부스팅 + 그리드서치로 모델 학습\n",
    "gb = GradientBoostingClassifier(random_state=1)\n",
    "param_grid = [{'n_estimators': range(5, 50, 10), 'max_features': range(1, 4),\n",
    "               'max_depth': range(3, 5), 'learning_rate': np.linspace(0.1, 1, 10)}]\n",
    "gs = GridSearchCV(estimator=gb, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "gs.fit(X_train_std, y_train)\n",
    "\n",
    "# 그리드서치 학습 결과 출력\n",
    "print('베스트 하이퍼 파라미터: {0}'.format(gs.best_params_))\n",
    "print('베스트 하이퍼 파라미터 일 때 정확도: {0:.2f}'.format(gs.best_score_))\n",
    "\n",
    "# 최적화 모델 추출\n",
    "model = gs.best_estimator_\n",
    "\n",
    "# 테스트세트 정확도 출력\n",
    "score = model.score(X_test_std, y_test)\n",
    "print('테스트세트에서의 정확도: {0:.2f}'.format(score))\n",
    "\n",
    "# 테스트세트 예측 결과 샘플 출력\n",
    "predicted_y = model.predict(X_test_std)\n",
    "for i in range(10):\n",
    "    print('실제 값: {0}, 예측 값: {1}'.format(labels[y_test[i]], labels[predicted_y[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "베스트 하이퍼 파라미터: {'n_neighbors': 8}\n",
      "베스트 하이퍼 파라미터 일 때 정확도: 0.96\n",
      "테스트세트에서의 정확도: 0.93\n",
      "실제 값: virginica, 예측 값: virginica\n",
      "실제 값: setosa, 예측 값: setosa\n",
      "실제 값: setosa, 예측 값: setosa\n",
      "실제 값: virginica, 예측 값: versicolor\n",
      "실제 값: versicolor, 예측 값: versicolor\n",
      "실제 값: versicolor, 예측 값: versicolor\n",
      "실제 값: virginica, 예측 값: virginica\n",
      "실제 값: versicolor, 예측 값: versicolor\n",
      "실제 값: virginica, 예측 값: virginica\n",
      "실제 값: setosa, 예측 값: setosa\n"
     ]
    }
   ],
   "source": [
    "#NearestNeighbor algorithms\n",
    "#유유상종, 새로운 데이터를 입력 받았을때 가장 가까이 있는 것이 무엇인지를 중심으로 새로운 데이터를 종류를 정해주는 알고리즘\n",
    "# -*- coding:utf-8 -*-\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 분류용 샘플 데이터 불러오기\n",
    "iris = load_iris()\n",
    "X, y, labels = iris.data, iris.target, iris.target_names\n",
    "\n",
    "# 학습/테스트 데이터셋 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1, stratify=y)\n",
    "\n",
    "# 데이터 전처리(표준화, Standardization)\n",
    "std = StandardScaler()\n",
    "X_train_std = std.fit_transform(X_train)\n",
    "X_test_std = std.transform(X_test)\n",
    "\n",
    "# KNN + 그리드서치로 모델 학습\n",
    "knn = KNeighborsClassifier()\n",
    "param_grid = [{'n_neighbors': range(3, 10)}]\n",
    "gs = GridSearchCV(estimator=knn, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "gs.fit(X_train_std, y_train)\n",
    "\n",
    "# 그리드서치 학습 결과 출력\n",
    "print('베스트 하이퍼 파라미터: {0}'.format(gs.best_params_))\n",
    "print('베스트 하이퍼 파라미터 일 때 정확도: {0:.2f}'.format(gs.best_score_))\n",
    "\n",
    "# 최적화 모델 추출\n",
    "model = gs.best_estimator_\n",
    "\n",
    "# 테스트세트 정확도 출력\n",
    "score = model.score(X_test_std, y_test)\n",
    "print('테스트세트에서의 정확도: {0:.2f}'.format(score))\n",
    "\n",
    "# 테스트세트 예측 결과 샘플 출력\n",
    "predicted_y = model.predict(X_test_std)\n",
    "for i in range(10):\n",
    "    print('실제 값: {0}, 예측 값: {1}'.format(labels[y_test[i]], labels[predicted_y[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "베스트 하이퍼 파라미터: {'C': 0.9, 'gamma': 0.1}\n",
      "베스트 하이퍼 파라미터 일 때 정확도: 0.96\n",
      "테스트세트에서의 정확도: 0.98\n",
      "실제 값: virginica, 예측 값: virginica\n",
      "실제 값: setosa, 예측 값: setosa\n",
      "실제 값: setosa, 예측 값: setosa\n",
      "실제 값: virginica, 예측 값: versicolor\n",
      "실제 값: versicolor, 예측 값: versicolor\n",
      "실제 값: versicolor, 예측 값: versicolor\n",
      "실제 값: virginica, 예측 값: virginica\n",
      "실제 값: versicolor, 예측 값: versicolor\n",
      "실제 값: virginica, 예측 값: virginica\n",
      "실제 값: setosa, 예측 값: setosa\n"
     ]
    }
   ],
   "source": [
    "#KernelSupportVector\n",
    "# -*- coding:utf-8 -*-\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# 분류용 샘플 데이터 불러오기\n",
    "iris = load_iris()\n",
    "X, y, labels = iris.data, iris.target, iris.target_names\n",
    "\n",
    "# 학습/테스트 데이터셋 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1, stratify=y)\n",
    "\n",
    "# 데이터 전처리(표준화, Standardization)\n",
    "std = StandardScaler()\n",
    "X_train_std = std.fit_transform(X_train)\n",
    "X_test_std = std.transform(X_test)\n",
    "\n",
    "# RBF 커널 서포트 벡터 머신 + 그리드서치로 모델 학습\n",
    "svm = SVC()\n",
    "param_grid = [{'C': np.linspace(0.1, 10, 100), 'gamma': np.linspace(0.1, 10, 100)}]\n",
    "gs = GridSearchCV(estimator=svm, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "gs.fit(X_train_std, y_train)\n",
    "\n",
    "# 그리드서치 학습 결과 출력\n",
    "print('베스트 하이퍼 파라미터: {0}'.format(gs.best_params_))\n",
    "print('베스트 하이퍼 파라미터 일 때 정확도: {0:.2f}'.format(gs.best_score_))\n",
    "\n",
    "# 최적화 모델 추출\n",
    "model = gs.best_estimator_\n",
    "\n",
    "# 테스트세트 정확도 출력\n",
    "score = model.score(X_test_std, y_test)\n",
    "print('테스트세트에서의 정확도: {0:.2f}'.format(score))\n",
    "\n",
    "# 테스트세트 예측 결과 샘플 출력\n",
    "predicted_y = model.predict(X_test_std)\n",
    "for i in range(10):\n",
    "    print('실제 값: {0}, 예측 값: {1}'.format(labels[y_test[i]], labels[predicted_y[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "베스트 하이퍼 파라미터: {'C': 3.1, 'penalty': 'l1'}\n",
      "베스트 하이퍼 파라미터 일 때 정확도: 0.93\n",
      "테스트세트에서의 정확도: 0.98\n",
      "실제 값: virginica, 예측 값: virginica\n",
      "실제 값: setosa, 예측 값: setosa\n",
      "실제 값: setosa, 예측 값: setosa\n",
      "실제 값: virginica, 예측 값: versicolor\n",
      "실제 값: versicolor, 예측 값: versicolor\n",
      "실제 값: versicolor, 예측 값: versicolor\n",
      "실제 값: virginica, 예측 값: virginica\n",
      "실제 값: versicolor, 예측 값: versicolor\n",
      "실제 값: virginica, 예측 값: virginica\n",
      "실제 값: setosa, 예측 값: setosa\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression Algorithms\n",
    "# -*- coding:utf-8 -*-\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# 분류용 샘플 데이터 불러오기\n",
    "iris = load_iris()\n",
    "X, y, labels = iris.data, iris.target, iris.target_names\n",
    "\n",
    "# 학습/테스트 데이터셋 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1, stratify=y)\n",
    "\n",
    "# 데이터 전처리(스케일 조정)\n",
    "scaler = RobustScaler()\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "X_test_std = scaler.transform(X_test)\n",
    "\n",
    "# 로지스틱 회귀 + 그리드서치로 모델 학습\n",
    "lg = LogisticRegression(solver='liblinear')\n",
    "param_grid = [{'C': np.linspace(0.1, 10, 100), 'penalty': ['l1', 'l2']}]\n",
    "gs = GridSearchCV(estimator=lg, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "gs.fit(X_train_std, y_train)\n",
    "\n",
    "# 그리드서치 학습 결과 출력\n",
    "print('베스트 하이퍼 파라미터: {0}'.format(gs.best_params_))\n",
    "print('베스트 하이퍼 파라미터 일 때 정확도: {0:.2f}'.format(gs.best_score_))\n",
    "\n",
    "# 최적화 모델 추출\n",
    "model = gs.best_estimator_\n",
    "\n",
    "# 테스트세트 정확도 출력\n",
    "score = model.score(X_test_std, y_test)\n",
    "print('테스트세트에서의 정확도: {0:.2f}'.format(score))\n",
    "\n",
    "# 테스트세트 예측 결과 샘플 출력\n",
    "predicted_y = model.predict(X_test_std)\n",
    "for i in range(10):\n",
    "    print('실제 값: {0}, 예측 값: {1}'.format(labels[y_test[i]], labels[predicted_y[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 569 entries, 0 to 568\n",
      "Data columns (total 31 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   mean radius              569 non-null    float64\n",
      " 1   mean texture             569 non-null    float64\n",
      " 2   mean perimeter           569 non-null    float64\n",
      " 3   mean area                569 non-null    float64\n",
      " 4   mean smoothness          569 non-null    float64\n",
      " 5   mean compactness         569 non-null    float64\n",
      " 6   mean concavity           569 non-null    float64\n",
      " 7   mean concave points      569 non-null    float64\n",
      " 8   mean symmetry            569 non-null    float64\n",
      " 9   mean fractal dimension   569 non-null    float64\n",
      " 10  radius error             569 non-null    float64\n",
      " 11  texture error            569 non-null    float64\n",
      " 12  perimeter error          569 non-null    float64\n",
      " 13  area error               569 non-null    float64\n",
      " 14  smoothness error         569 non-null    float64\n",
      " 15  compactness error        569 non-null    float64\n",
      " 16  concavity error          569 non-null    float64\n",
      " 17  concave points error     569 non-null    float64\n",
      " 18  symmetry error           569 non-null    float64\n",
      " 19  fractal dimension error  569 non-null    float64\n",
      " 20  worst radius             569 non-null    float64\n",
      " 21  worst texture            569 non-null    float64\n",
      " 22  worst perimeter          569 non-null    float64\n",
      " 23  worst area               569 non-null    float64\n",
      " 24  worst smoothness         569 non-null    float64\n",
      " 25  worst compactness        569 non-null    float64\n",
      " 26  worst concavity          569 non-null    float64\n",
      " 27  worst concave points     569 non-null    float64\n",
      " 28  worst symmetry           569 non-null    float64\n",
      " 29  worst fractal dimension  569 non-null    float64\n",
      " 30  target                   569 non-null    int32  \n",
      "dtypes: float64(30), int32(1)\n",
      "memory usage: 135.7 KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method IndexOpsMixin.value_counts of 0      0\n",
       "1      0\n",
       "2      0\n",
       "3      0\n",
       "4      0\n",
       "      ..\n",
       "564    0\n",
       "565    0\n",
       "566    0\n",
       "567    0\n",
       "568    1\n",
       "Name: target, Length: 569, dtype: int32>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Support Vector Machine  \n",
    "import sklearn.datasets as d\n",
    "import pandas as pd\n",
    "#breast_cancer 데이터 셋 로드\n",
    "x = d.load_breast_cancer()\n",
    "cancer = pd.DataFrame(data = x.data, columns = x.feature_names)\n",
    "cancer['target'] = x.target\n",
    "\n",
    "cancer.info()\n",
    "cancer.describe()\n",
    "cancer.target.value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "교차검증 평균:  0.9800000000000001\n"
     ]
    }
   ],
   "source": [
    "import sklearn.svm as svm\n",
    "import sklearn.metrics as mt\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "\n",
    "# SVM, kernel = 'linear'로 선형분리 진행\n",
    " \n",
    "svm_clf =svm.SVC(kernel = 'linear')\n",
    "\n",
    "# 교차검증\n",
    "\n",
    "scores = cross_val_score(svm_clf, X, y, cv = 5)\n",
    "scores\n",
    "\n",
    "pd.DataFrame(cross_validate(svm_clf, X, y, cv =5))\n",
    "\n",
    "print('교차검증 평균: ', scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "교차검증 평균:  0.9666666666666666\n"
     ]
    }
   ],
   "source": [
    "# SVM, kernel = 'rbf'로 비선형분리 진행\n",
    " \n",
    "svm_clf =svm.SVC(kernel = 'rbf')\n",
    "\n",
    "# 교차검증\n",
    "\n",
    "scores = cross_val_score(svm_clf, X, y, cv = 5)\n",
    "scores\n",
    "\n",
    "pd.DataFrame(cross_validate(svm_clf, X, y, cv =5))\n",
    "\n",
    "print('교차검증 평균: ', scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score :  0.9666666666666667\n",
      "[[12  0  0]\n",
      " [ 0  8  1]\n",
      " [ 0  0  9]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        12\n",
      "           1       1.00      0.89      0.94         9\n",
      "           2       0.90      1.00      0.95         9\n",
      "\n",
      "    accuracy                           0.97        30\n",
      "   macro avg       0.97      0.96      0.96        30\n",
      "weighted avg       0.97      0.97      0.97        30\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CNC Aid\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "#Support Vector machine Example\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split  \n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC  \n",
    "from sklearn.metrics import classification_report, confusion_matrix  \n",
    "\n",
    "iris = load_iris()\n",
    "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "y = pd.DataFrame({'class':iris.target})\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)  \n",
    "\n",
    "svclassifier = SVC(kernel='linear', degree=8)  \n",
    "svclassifier.fit(X_train, y_train) \n",
    "\n",
    "y_pred = svclassifier.predict(X_test)  \n",
    "print('score : ',svclassifier.score(X_test,y_test))\n",
    "print(confusion_matrix(y_test, y_pred))  \n",
    "print(classification_report(y_test, y_pred))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
